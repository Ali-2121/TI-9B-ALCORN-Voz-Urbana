{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b54c393",
   "metadata": {},
   "source": [
    "# Modelo de Apendizaje no Supervisado Avanzado de Detección de Zonas Críticas\n",
    "\n",
    "Este notebook implementa un modelo avanzado para detectar zonas críticas usando clustering DBSCAN con:\n",
    "- Ingeniería de características avanzada\n",
    "- Optimización automática de parámetros\n",
    "- Validación de calidad con métricas\n",
    "- Análisis temporal y categórico\n",
    "- Visualización completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_visualizacion_completa(df_resultados, analisis_zonas, n_clusters, grafico_path):\n",
    "    \"\"\"Versión minimalista - solo mapa de zonas críticas\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    fig.suptitle(f'Zonas Críticas Detectadas: {n_clusters}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if not df_resultados.empty:\n",
    "        scatter = ax.scatter(\n",
    "            df_resultados['longitud'],\n",
    "            df_resultados['latitud'],\n",
    "            c=df_resultados['cluster'],\n",
    "            cmap='viridis',\n",
    "            s=50,\n",
    "            alpha=0.7,\n",
    "            edgecolors='white',\n",
    "            linewidth=0.3\n",
    "        )\n",
    "        \n",
    "        if not analisis_zonas.empty:\n",
    "            ax.scatter(\n",
    "                analisis_zonas['lng_centro'], \n",
    "                analisis_zonas['lat_centro'],\n",
    "                marker='*',\n",
    "                s=400,\n",
    "                color='red',\n",
    "                edgecolors='black',\n",
    "                linewidth=2,\n",
    "                label='Centros de Zonas Críticas',\n",
    "                zorder=5\n",
    "            )\n",
    "            \n",
    "            for idx, zona in analisis_zonas.iterrows():\n",
    "                ax.annotate(\n",
    "                    f'Z{idx}', \n",
    "                    (zona['lng_centro'], zona['lat_centro']),\n",
    "                    xytext=(5, 5), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10, \n",
    "                    fontweight='bold',\n",
    "                    color='white',\n",
    "                    bbox=dict(boxstyle='circle,pad=0.3', facecolor='red', alpha=0.8)\n",
    "                )\n",
    "        \n",
    "        ax.set_title('Mapa de Reportes de Alta Prioridad', fontsize=14, pad=20)\n",
    "        ax.set_xlabel('Longitud', fontsize=12)\n",
    "        ax.set_ylabel('Latitud', fontsize=12)\n",
    "        \n",
    "        cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "        cbar.set_label('Cluster ID (-1 = Ruido)', fontsize=10)\n",
    "        \n",
    "        if not analisis_zonas.empty:\n",
    "            ax.legend(loc='upper right', fontsize=10)\n",
    "        \n",
    "        ax.grid(True, alpha=0.2)\n",
    "        \n",
    "        # Información estadística\n",
    "        n_reportes_en_zonas = len(df_resultados[df_resultados['cluster'] != -1])\n",
    "        cobertura = (n_reportes_en_zonas / len(df_resultados) * 100) if len(df_resultados) > 0 else 0\n",
    "        info_text = f\"Reportes: {len(df_resultados):,} | Zonas: {n_clusters} | Cobertura: {cobertura:.1f}%\"\n",
    "        \n",
    "        ax.text(\n",
    "            0.02, 0.02, info_text, \n",
    "            transform=ax.transAxes, \n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No hay datos para mostrar', ha='center', va='center', fontsize=16)\n",
    "        ax.set_title('Sin datos disponibles')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(grafico_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()  # Mostrar en el notebook\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c05fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_zonas_criticas_avanzado(input_csv, auto_params=True, eps=None, min_samples=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Modelo avanzado de detección de zonas críticas con:\n",
    "    - Ingeniería de características avanzada\n",
    "    - Optimización automática de parámetros\n",
    "    - Validación de calidad con métricas\n",
    "    - Análisis temporal y categórico\n",
    "    - Visualización completa\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "\n",
    "    try:\n",
    "        log_print(f\"\\n=== MODELO AVANZADO DE DETECCIÓN DE ZONAS CRÍTICAS ===\")\n",
    "        log_print(f\"[ETAPA 1/6] Cargando y validando datos...\")\n",
    "\n",
    "        if not os.path.exists(input_csv):\n",
    "            raise FileNotFoundError(f\"Archivo no encontrado: {input_csv}\")\n",
    "\n",
    "        df = pd.read_csv(input_csv)\n",
    "        log_print(f\"✓ Datos cargados. Total de registros: {len(df)}\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"El archivo CSV está vacío\")\n",
    "        \n",
    "        columnas_requeridas = ['latitud', 'longitud', 'prioridad']\n",
    "        columnas_faltantes = [col for col in columnas_requeridas if col not in df.columns]\n",
    "        if columnas_faltantes:\n",
    "            raise ValueError(f\"Columnas faltantes: {columnas_faltantes}\")\n",
    "\n",
    "        df_alta = df[df['prioridad'] == 'alta'].copy()\n",
    "        log_print(f\"✓ Reportes de alta prioridad: {len(df_alta)}\")\n",
    "        \n",
    "        if len(df_alta) < 10:\n",
    "            raise ValueError(f\"Insuficientes reportes de alta prioridad: {len(df_alta)} (mínimo 10)\")\n",
    "\n",
    "        # Limpieza de datos\n",
    "        df_alta = df_alta[\n",
    "            (df_alta['latitud'].notna()) & \n",
    "            (df_alta['longitud'].notna()) &\n",
    "            (df_alta['latitud'] != 'Latitud no especificada') &\n",
    "            (df_alta['longitud'] != 'Longitud no especificada')\n",
    "        ].copy()\n",
    "        \n",
    "        df_alta['latitud'] = pd.to_numeric(df_alta['latitud'], errors='coerce')\n",
    "        df_alta['longitud'] = pd.to_numeric(df_alta['longitud'], errors='coerce')\n",
    "        df_alta = df_alta.dropna(subset=['latitud', 'longitud'])\n",
    "        \n",
    "        log_print(f\"✓ Datos limpios: {len(df_alta)} reportes válidos\")\n",
    "\n",
    "        return df_alta  # Retornamos los datos procesados para continuar en la siguiente celda\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_details = {\n",
    "            \"error\": str(e),\n",
    "            \"tipo\": type(e).__name__,\n",
    "            \"archivo\": input_csv,\n",
    "            \"traceback\": traceback.format_exc()\n",
    "        }\n",
    "        log_print(f\"\\n❌ ERROR CRÍTICO:\\n{json.dumps(error_details, indent=2)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_caracteristicas_avanzadas(df_alta, verbose=True):\n",
    "    \"\"\"\n",
    "    Genera características avanzadas para el modelo\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 2/6] Generando características avanzadas...\")\n",
    "    \n",
    "    df_features = df_alta[['latitud', 'longitud']].copy()\n",
    "    \n",
    "    # Características temporales\n",
    "    if 'fecha_creacion' in df_alta.columns:\n",
    "        df_alta['fecha_creacion'] = pd.to_datetime(df_alta['fecha_creacion'], errors='coerce')\n",
    "        df_features['hora_reporte'] = df_alta['fecha_creacion'].dt.hour\n",
    "        df_features['dia_semana'] = df_alta['fecha_creacion'].dt.dayofweek\n",
    "        df_features['mes'] = df_alta['fecha_creacion'].dt.month\n",
    "        df_features['es_horario_laboral'] = ((df_features['hora_reporte'] >= 8) & \n",
    "                                           (df_features['hora_reporte'] <= 18)).astype(int)\n",
    "        df_features['es_fin_semana'] = (df_features['dia_semana'] >= 5).astype(int)\n",
    "    \n",
    "    # Características categóricas\n",
    "    if 'categoria_id' in df_alta.columns:\n",
    "        df_alta['categoria_id'] = pd.to_numeric(df_alta['categoria_id'], errors='coerce').fillna(0)\n",
    "        le_categoria = LabelEncoder()\n",
    "        df_features['categoria_encoded'] = le_categoria.fit_transform(df_alta['categoria_id'])\n",
    "    \n",
    "    if 'estado' in df_alta.columns:\n",
    "        le_estado = LabelEncoder()\n",
    "        df_features['estado_encoded'] = le_estado.fit_transform(df_alta['estado'].fillna('nuevo'))\n",
    "    \n",
    "    # Características espaciales\n",
    "    coords_temp = df_features[['latitud', 'longitud']].values\n",
    "    k_neighbors = min(10, len(coords_temp) - 1)\n",
    "    \n",
    "    if k_neighbors > 0:\n",
    "        nbrs = NearestNeighbors(n_neighbors=k_neighbors, radius=0.01).fit(coords_temp)\n",
    "        distances, indices = nbrs.kneighbors(coords_temp)\n",
    "        df_features['densidad_local'] = np.mean(distances, axis=1)\n",
    "        df_features['num_vecinos_cercanos'] = np.sum(distances < 0.005, axis=1)\n",
    "    \n",
    "    # Distancia al centro\n",
    "    centro_ciudad = [df_features['latitud'].mean(), df_features['longitud'].mean()]\n",
    "    df_features['distancia_centro'] = np.sqrt(\n",
    "        (df_features['latitud'] - centro_ciudad[0])**2 + \n",
    "        (df_features['longitud'] - centro_ciudad[1])**2\n",
    "    )\n",
    "    \n",
    "    # Normalización de coordenadas\n",
    "    df_features['lat_normalizada'] = (df_features['latitud'] - df_features['latitud'].min()) / (df_features['latitud'].max() - df_features['latitud'].min())\n",
    "    df_features['lng_normalizada'] = (df_features['longitud'] - df_features['longitud'].min()) / (df_features['longitud'].max() - df_features['longitud'].min())\n",
    "    \n",
    "    log_print(f\"✓ Características generadas: {df_features.shape[1]} variables\")\n",
    "    \n",
    "    # Escalado de características\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(df_features)\n",
    "    log_print(f\"✓ Características normalizadas\")\n",
    "    \n",
    "    return df_features, features_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizar_parametros_dbscan(features_scaled, auto_params=True, eps=None, min_samples=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimiza los parámetros de DBSCAN\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 3/6] Optimizando parámetros...\")\n",
    "    \n",
    "    if auto_params:\n",
    "        # Análisis de k-distance\n",
    "        k = min(max(4, len(features_scaled) // 50), 15)\n",
    "        nbrs = NearestNeighbors(n_neighbors=k).fit(features_scaled)\n",
    "        distances, indices = nbrs.kneighbors(features_scaled)\n",
    "        k_distances = np.sort(distances[:, k-1], axis=0)\n",
    "        \n",
    "        eps_candidates = [\n",
    "            np.percentile(k_distances, 85),\n",
    "            np.percentile(k_distances, 90),\n",
    "            np.percentile(k_distances, 95)\n",
    "        ]\n",
    "        \n",
    "        min_samples_candidates = [\n",
    "            max(3, len(features_scaled) // 100),\n",
    "            max(5, len(features_scaled) // 50),\n",
    "            max(7, len(features_scaled) // 30)\n",
    "        ]\n",
    "        \n",
    "        log_print(f\"✓ Candidatos eps: {[f'{x:.4f}' for x in eps_candidates]}\")\n",
    "        log_print(f\"✓ Candidatos min_samples: {min_samples_candidates}\")\n",
    "    else:\n",
    "        eps_candidates = [eps or 0.5]\n",
    "        min_samples_candidates = [min_samples or 5]\n",
    "        log_print(f\"✓ Parámetros manuales: eps={eps_candidates[0]}, min_samples={min_samples_candidates[0]}\")\n",
    "    \n",
    "    return eps_candidates, min_samples_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_clustering(features_scaled, eps_candidates, min_samples_candidates, verbose=True):\n",
    "    \"\"\"\n",
    "    Ejecuta el clustering DBSCAN con validación\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 4/6] Ejecutando clustering con validación...\")\n",
    "    \n",
    "    best_score = -1\n",
    "    best_labels = None\n",
    "    best_params = None\n",
    "    best_dbscan = None\n",
    "    resultados_pruebas = []\n",
    "    \n",
    "    for test_eps in eps_candidates:\n",
    "        for test_min_samples in min_samples_candidates:\n",
    "            try:\n",
    "                dbscan_test = DBSCAN(eps=test_eps, min_samples=test_min_samples, metric='euclidean')\n",
    "                labels_test = dbscan_test.fit_predict(features_scaled)\n",
    "                \n",
    "                n_clusters_test = len(set(labels_test)) - (1 if -1 in labels_test else 0)\n",
    "                n_noise_test = list(labels_test).count(-1)\n",
    "                \n",
    "                if n_clusters_test >= 1 and n_clusters_test <= len(features_scaled) // 3:\n",
    "                    if n_clusters_test == 1:\n",
    "                        score = 1.0 - (n_noise_test / len(labels_test))\n",
    "                    else:\n",
    "                        try:\n",
    "                            score = silhouette_score(features_scaled, labels_test)\n",
    "                        except:\n",
    "                            score = 0.0\n",
    "                    \n",
    "                    resultado = {\n",
    "                        'eps': test_eps,\n",
    "                        'min_samples': test_min_samples,\n",
    "                        'n_clusters': n_clusters_test,\n",
    "                        'n_noise': n_noise_test,\n",
    "                        'score': score\n",
    "                    }\n",
    "                    resultados_pruebas.append(resultado)\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_labels = labels_test\n",
    "                        best_params = (test_eps, test_min_samples)\n",
    "                        best_dbscan = dbscan_test\n",
    "                        \n",
    "            except Exception as e:\n",
    "                log_print(f\"  Error con eps={test_eps:.4f}, min_samples={test_min_samples}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Configuración fallback si no se encuentra una buena solución\n",
    "    if best_labels is None:\n",
    "        log_print(\"  ⚠  Aplicando configuración fallback...\")\n",
    "        fallback_eps = 0.5\n",
    "        fallback_min_samples = max(3, min(10, len(features_scaled) // 20))\n",
    "        best_dbscan = DBSCAN(eps=fallback_eps, min_samples=fallback_min_samples)\n",
    "        best_labels = best_dbscan.fit_predict(features_scaled)\n",
    "        best_params = (fallback_eps, fallback_min_samples)\n",
    "        best_score = 0.0\n",
    "\n",
    "    n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)\n",
    "    n_noise = list(best_labels).count(-1)\n",
    "    \n",
    "    log_print(f\"✓ Mejor configuración encontrada:\")\n",
    "    log_print(f\"  - Parámetros: eps={best_params[0]:.4f}, min_samples={best_params[1]}\")\n",
    "    log_print(f\"  - Zonas críticas detectadas: {n_clusters}\")\n",
    "    log_print(f\"  - Reportes aislados: {n_noise}\")\n",
    "    log_print(f\"  - Score de calidad: {best_score:.3f}\")\n",
    "    \n",
    "    return best_dbscan, best_labels, best_params, best_score, resultados_pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ca097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_resultados(df_alta, best_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Analiza los resultados del clustering\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 5/6] Analizando resultados...\")\n",
    "    \n",
    "    df_resultados = df_alta.copy()\n",
    "    df_resultados['cluster'] = best_labels\n",
    "    df_resultados['es_zona_critica'] = df_resultados['cluster'] != -1\n",
    "    \n",
    "    zonas_criticas = df_resultados[df_resultados['es_zona_critica']]\n",
    "    \n",
    "    if not zonas_criticas.empty:\n",
    "        # Estadísticas básicas por zona\n",
    "        stats_basicos = zonas_criticas.groupby('cluster').agg({\n",
    "            'latitud': ['mean', 'std', 'min', 'max', 'count'],\n",
    "            'longitud': ['mean', 'std', 'min', 'max'],\n",
    "        }).round(6)\n",
    "        \n",
    "        # Estadísticas adicionales\n",
    "        stats_adicionales = {}\n",
    "        \n",
    "        if 'categoria_id' in zonas_criticas.columns:\n",
    "            stats_adicionales['categoria_dominante'] = zonas_criticas.groupby('cluster')['categoria_id'].agg(\n",
    "                lambda x: Counter(x).most_common(1)[0][0] if len(x) > 0 else None\n",
    "            )\n",
    "        \n",
    "        if 'estado' in zonas_criticas.columns:\n",
    "            stats_adicionales['estado_dominante'] = zonas_criticas.groupby('cluster')['estado'].agg(\n",
    "                lambda x: Counter(x).most_common(1)[0][0] if len(x) > 0 else None\n",
    "            )\n",
    "        \n",
    "        if 'fecha_creacion' in zonas_criticas.columns:\n",
    "            stats_adicionales['periodo'] = zonas_criticas.groupby('cluster')['fecha_creacion'].agg(['min', 'max'])\n",
    "        \n",
    "        # Crear DataFrame de análisis\n",
    "        analisis_zonas = pd.DataFrame({\n",
    "            'lat_centro': stats_basicos['latitud']['mean'],\n",
    "            'lng_centro': stats_basicos['longitud']['mean'],\n",
    "            'lat_std': stats_basicos['latitud']['std'].fillna(0),\n",
    "            'lng_std': stats_basicos['longitud']['std'].fillna(0),\n",
    "            'num_reportes': stats_basicos['latitud']['count'],\n",
    "            'lat_min': stats_basicos['latitud']['min'],\n",
    "            'lat_max': stats_basicos['latitud']['max'],\n",
    "            'lng_min': stats_basicos['longitud']['min'],\n",
    "            'lng_max': stats_basicos['longitud']['max']\n",
    "        })\n",
    "        \n",
    "        # Agregar estadísticas adicionales\n",
    "        for key, values in stats_adicionales.items():\n",
    "            if isinstance(values, pd.DataFrame):\n",
    "                for col in values.columns:\n",
    "                    analisis_zonas[f'{key}_{col}'] = values[col]\n",
    "            else:\n",
    "                analisis_zonas[key] = values\n",
    "        \n",
    "        # Métricas de criticidad\n",
    "        analisis_zonas['densidad'] = analisis_zonas['num_reportes']\n",
    "        analisis_zonas['area_cobertura'] = (analisis_zonas['lat_max'] - analisis_zonas['lat_min']) * (analisis_zonas['lng_max'] - analisis_zonas['lng_min'])\n",
    "        analisis_zonas['compacidad'] = analisis_zonas['lat_std'] + analisis_zonas['lng_std']\n",
    "        \n",
    "        analisis_zonas['score_criticidad'] = (\n",
    "            (analisis_zonas['densidad'] / analisis_zonas['densidad'].max()) * 0.5 +\n",
    "            (1 / (analisis_zonas['compacidad'] + 0.001)) * 0.3 +\n",
    "            (1 / (analisis_zonas['area_cobertura'] + 0.001)) * 0.2\n",
    "        )\n",
    "        \n",
    "        # Clasificación de criticidad\n",
    "        if len(analisis_zonas) > 1:\n",
    "            try:\n",
    "                analisis_zonas['nivel_criticidad'] = pd.qcut(\n",
    "                    analisis_zonas['score_criticidad'], \n",
    "                    q=min(3, len(analisis_zonas)),\n",
    "                    labels=['Media', 'Alta', 'Crítica'][:min(3, len(analisis_zonas))],\n",
    "                    duplicates='drop'\n",
    "                )\n",
    "            except:\n",
    "                analisis_zonas['nivel_criticidad'] = 'Alta'\n",
    "        else:\n",
    "            analisis_zonas['nivel_criticidad'] = 'Alta'\n",
    "        \n",
    "        log_print(f\"✓ Análisis completado - Resumen por zona:\")\n",
    "        for idx, zona in analisis_zonas.iterrows():\n",
    "            categoria_info = f\", Cat. dominante: {zona.get('categoria_dominante', 'N/A')}\" if 'categoria_dominante' in zona else \"\"\n",
    "            log_print(f\"  Zona {idx}: {zona['num_reportes']} reportes, Centro: ({zona['lat_centro']:.4f}, {zona['lng_centro']:.4f})\")\n",
    "            log_print(f\"    Nivel: {zona['nivel_criticidad']}, Score: {zona['score_criticidad']:.2f}{categoria_info}\")\n",
    "    else:\n",
    "        analisis_zonas = pd.DataFrame()\n",
    "        log_print(\"  ℹ  No se identificaron zonas críticas\")\n",
    "    \n",
    "    return df_resultados, analisis_zonas, zonas_criticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4726566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_modelo_y_generar_visualizacion(best_dbscan, scaler, df_features, analisis_zonas, \n",
    "                                          resultados_pruebas, best_score, best_params, \n",
    "                                          df_resultados, n_clusters, verbose=True):\n",
    "    \"\"\"\n",
    "    Guarda el modelo y genera la visualización final\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 6/6] Generando visualización y guardando modelo...\")\n",
    "    \n",
    "    # Crear directorios\n",
    "    os.makedirs(\"modelos\", exist_ok=True)\n",
    "    os.makedirs(\"graficos\", exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Métricas de validación\n",
    "    zonas_criticas = df_resultados[df_resultados['cluster'] != -1]\n",
    "    n_noise = list(df_resultados['cluster']).count(-1)\n",
    "    \n",
    "    metricas_validacion = {\n",
    "        'silhouette_score': float(best_score),\n",
    "        'n_clusters': int(n_clusters),\n",
    "        'n_noise': int(n_noise),\n",
    "        'coverage_ratio': float(len(zonas_criticas) / len(df_resultados)) if len(df_resultados) > 0 else 0.0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if n_clusters > 1:\n",
    "            features_scaled = StandardScaler().fit_transform(df_features)\n",
    "            metricas_validacion['calinski_harabasz'] = float(\n",
    "                calinski_harabasz_score(features_scaled, df_resultados['cluster'])\n",
    "            )\n",
    "    except:\n",
    "        metricas_validacion['calinski_harabasz'] = 0.0\n",
    "    \n",
    "    # Preparar datos del modelo\n",
    "    modelo_data = {\n",
    "        'dbscan': best_dbscan,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': df_features.columns.tolist(),\n",
    "        'zonas_criticas': analisis_zonas,\n",
    "        'resultados_optimizacion': resultados_pruebas,\n",
    "        'metricas_validacion': metricas_validacion,\n",
    "        'metadata': {\n",
    "            'fecha_entrenamiento': timestamp,\n",
    "            'version': 'solo_no_supervisado_v1',\n",
    "            'caracteristicas_usadas': len(df_features.columns),\n",
    "            'parametros_finales': {\n",
    "                'eps': best_params[0],\n",
    "                'min_samples': best_params[1]\n",
    "            },\n",
    "            'estadisticas': {\n",
    "                'reportes_procesados': len(df_resultados),\n",
    "                'zonas_criticas_detectadas': n_clusters,\n",
    "                'reportes_en_zonas_criticas': len(zonas_criticas),\n",
    "                'reportes_aislados': n_noise\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Rutas de archivos\n",
    "    modelo_path = os.path.join(\"modelos\", f\"zonas_criticas_avanzado_{timestamp}.joblib\")\n",
    "    grafico_path = os.path.join(\"graficos\", f\"zonas_criticas_avanzado_{timestamp}.png\")\n",
    "    \n",
    "    # Generar visualización\n",
    "    generar_visualizacion_completa(df_resultados, analisis_zonas, n_clusters, grafico_path)\n",
    "    log_print(f\"✓ Visualización completa generada: {grafico_path}\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    joblib.dump(modelo_data, modelo_path)\n",
    "    log_print(f\"✅ Modelo completo guardado: {modelo_path}\")\n",
    "    \n",
    "    # Resumen final\n",
    "    log_print(f\"\\n=== ENTRENAMIENTO COMPLETADO EXITOSAMENTE ===\")\n",
    "    log_print(f\"Zonas críticas detectadas: {n_clusters}\")\n",
    "    log_print(f\"Calidad del modelo: {best_score:.3f}\")\n",
    "    log_print(f\"Cobertura de reportes: {(len(zonas_criticas)/len(df_resultados)*100):.1f}%\")\n",
    "    \n",
    "    return modelo_path, grafico_path, metricas_validacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea275018",
   "metadata": {},
   "source": [
    "## Función Principal Integrada\n",
    "\n",
    "Esta función integra todo el pipeline de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd976bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_completo(input_csv, auto_params=True, eps=None, min_samples=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta todo el pipeline de entrenamiento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Etapa 1: Cargar y validar datos\n",
    "        df_alta = entrenar_modelo_zonas_criticas_avanzado(input_csv, auto_params, eps, min_samples, verbose)\n",
    "        \n",
    "        # Etapa 2: Generar características\n",
    "        df_features, features_scaled, scaler = generar_caracteristicas_avanzadas(df_alta, verbose)\n",
    "        \n",
    "        # Etapa 3: Optimizar parámetros\n",
    "        eps_candidates, min_samples_candidates = optimizar_parametros_dbscan(\n",
    "            features_scaled, auto_params, eps, min_samples, verbose\n",
    "        )\n",
    "        \n",
    "        # Etapa 4: Ejecutar clustering\n",
    "        best_dbscan, best_labels, best_params, best_score, resultados_pruebas = ejecutar_clustering(\n",
    "            features_scaled, eps_candidates, min_samples_candidates, verbose\n",
    "        )\n",
    "        \n",
    "        # Etapa 5: Analizar resultados\n",
    "        df_resultados, analisis_zonas, zonas_criticas = analizar_resultados(df_alta, best_labels, verbose)\n",
    "        \n",
    "        # Calcular número de clusters\n",
    "        n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)\n",
    "        \n",
    "        # Etapa 6: Guardar modelo y generar visualización\n",
    "        modelo_path, grafico_path, metricas_validacion = guardar_modelo_y_generar_visualizacion(\n",
    "            best_dbscan, scaler, df_features, analisis_zonas, resultados_pruebas, \n",
    "            best_score, best_params, df_resultados, n_clusters, verbose\n",
    "        )\n",
    "        \n",
    "        # Retornar resultados\n",
    "        resultado = {\n",
    "            \"success\": True,\n",
    "            \"modelo_path\": modelo_path,\n",
    "            \"grafico_path\": grafico_path,\n",
    "            \"version\": \"solo_no_supervisado_v1\",\n",
    "            \"metricas\": metricas_validacion,\n",
    "            \"analisis_zonas\": analisis_zonas,\n",
    "            \"df_resultados\": df_resultados\n",
    "        }\n",
    "        \n",
    "        return resultado\n",
    "        \n",
    "    except Exception as e:\n",
    "        resultado_error = {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"version\": \"solo_no_supervisado_v1\"\n",
    "        }\n",
    "        print(f\"Error en el entrenamiento: {resultado_error}\")\n",
    "        return resultado_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f10bcc",
   "metadata": {},
   "source": [
    "## Ejemplo de Uso\n",
    "\n",
    "Aquí puedes ejecutar el modelo con tus datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso del modelo\n",
    "# Cambia 'tu_archivo.csv' por la ruta de tu archivo de datos\n",
    "\n",
    "# Configuración\n",
    "archivo_datos = 'tu_archivo.csv'  # Cambia por tu archivo\n",
    "usar_optimizacion_automatica = True  # True para optimización automática, False para parámetros manuales\n",
    "eps_manual = None  # Solo si usar_optimizacion_automatica = False\n",
    "min_samples_manual = None  # Solo si usar_optimizacion_automatica = False\n",
    "\n",
    "# Ejecutar el modelo\n",
    "if os.path.exists(archivo_datos):\n",
    "    print(\"Iniciando entrenamiento del modelo...\")\n",
    "    resultado = entrenar_modelo_completo(\n",
    "        input_csv=archivo_datos,\n",
    "        auto_params=usar_optimizacion_automatica,\n",
    "        eps=eps_manual,\n",
    "        min_samples=min_samples_manual,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    if resultado['success']:\n",
    "        print(\"\\n🎉 ¡Entrenamiento completado exitosamente!\")\n",
    "        print(f\"📊 Modelo guardado en: {resultado['modelo_path']}\")\n",
    "        print(f\"📈 Gráfico generado en: {resultado['grafico_path']}\")\n",
    "        \n",
    "        # Mostrar métricas\n",
    "        print(\"\\n📋 Métricas del modelo:\")\n",
    "        for metrica, valor in resultado['metricas'].items():\n",
    "            print(f\"  {metrica}: {valor}\")\n",
    "        \n",
    "        # Mostrar resumen de zonas críticas\n",
    "        if not resultado['analisis_zonas'].empty:\n",
    "            print(\"\\n🗺️ Resumen de zonas críticas:\")\n",
    "            print(resultado['analisis_zonas'][['num_reportes', 'nivel_criticidad', 'score_criticidad']].head())\n",
    "    else:\n",
    "        print(f\"❌ Error en el entrenamiento: {resultado['error']}\")\n",
    "else:\n",
    "    print(f\"❌ Archivo no encontrado: {archivo_datos}\")\n",
    "    print(\"💡 Asegúrate de que el archivo CSV existe y contiene las columnas: 'latitud', 'longitud', 'prioridad'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718747ba",
   "metadata": {},
   "source": [
    "## Carga y Análisis de Modelo Previamente Entrenado\n",
    "\n",
    "Si ya tienes un modelo entrenado, puedes cargarlo y analizarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d456ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_y_analizar_modelo(modelo_path):\n",
    "    \"\"\"\n",
    "    Carga un modelo previamente entrenado y muestra su análisis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar modelo\n",
    "        modelo_data = joblib.load(modelo_path)\n",
    "        \n",
    "        print(f\"📂 Modelo cargado desde: {modelo_path}\")\n",
    "        print(f\"📅 Fecha de entrenamiento: {modelo_data['metadata']['fecha_entrenamiento']}\")\n",
    "        print(f\"🔧 Versión: {modelo_data['metadata']['version']}\")\n",
    "        \n",
    "        # Mostrar estadísticas\n",
    "        stats = modelo_data['metadata']['estadisticas']\n",
    "        print(\"\\n📊 Estadísticas del modelo:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Mostrar métricas de validación\n",
    "        print(\"\\n📈 Métricas de validación:\")\n",
    "        for key, value in modelo_data['metricas_validacion'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Mostrar zonas críticas\n",
    "        if not modelo_data['zonas_criticas'].empty:\n",
    "            print(\"\\n🗺️ Zonas críticas detectadas:\")\n",
    "            zonas = modelo_data['zonas_criticas']\n",
    "            for idx, zona in zonas.iterrows():\n",
    "                print(f\"  Zona {idx}: {zona['num_reportes']} reportes, Nivel: {zona['nivel_criticidad']}\")\n",
    "                print(f\"    Centro: ({zona['lat_centro']:.4f}, {zona['lng_centro']:.4f})\")\n",
    "                print(f\"    Score de criticidad: {zona['score_criticidad']:.2f}\")\n",
    "        \n",
    "        return modelo_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando el modelo: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejemplo de uso\n",
    "# modelo_path = \"modelos/zonas_criticas_avanzado_20241212_143022.joblib\" \n",
    "# modelo_cargado = cargar_y_analizar_modelo(modelo_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
