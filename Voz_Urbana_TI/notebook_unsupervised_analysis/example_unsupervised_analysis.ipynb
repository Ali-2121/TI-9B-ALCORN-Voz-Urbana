{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b54c393",
   "metadata": {},
   "source": [
    "# Modelo de Apendizaje no Supervisado Avanzado de Detecci√≥n de Zonas Cr√≠ticas\n",
    "\n",
    "Este notebook implementa un modelo avanzado para detectar zonas cr√≠ticas usando clustering DBSCAN con:\n",
    "- Ingenier√≠a de caracter√≠sticas avanzada\n",
    "- Optimizaci√≥n autom√°tica de par√°metros\n",
    "- Validaci√≥n de calidad con m√©tricas\n",
    "- An√°lisis temporal y categ√≥rico\n",
    "- Visualizaci√≥n completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_visualizacion_completa(df_resultados, analisis_zonas, n_clusters, grafico_path):\n",
    "    \"\"\"Versi√≥n minimalista - solo mapa de zonas cr√≠ticas\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    fig.suptitle(f'Zonas Cr√≠ticas Detectadas: {n_clusters}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if not df_resultados.empty:\n",
    "        scatter = ax.scatter(\n",
    "            df_resultados['longitud'],\n",
    "            df_resultados['latitud'],\n",
    "            c=df_resultados['cluster'],\n",
    "            cmap='viridis',\n",
    "            s=50,\n",
    "            alpha=0.7,\n",
    "            edgecolors='white',\n",
    "            linewidth=0.3\n",
    "        )\n",
    "        \n",
    "        if not analisis_zonas.empty:\n",
    "            ax.scatter(\n",
    "                analisis_zonas['lng_centro'], \n",
    "                analisis_zonas['lat_centro'],\n",
    "                marker='*',\n",
    "                s=400,\n",
    "                color='red',\n",
    "                edgecolors='black',\n",
    "                linewidth=2,\n",
    "                label='Centros de Zonas Cr√≠ticas',\n",
    "                zorder=5\n",
    "            )\n",
    "            \n",
    "            for idx, zona in analisis_zonas.iterrows():\n",
    "                ax.annotate(\n",
    "                    f'Z{idx}', \n",
    "                    (zona['lng_centro'], zona['lat_centro']),\n",
    "                    xytext=(5, 5), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10, \n",
    "                    fontweight='bold',\n",
    "                    color='white',\n",
    "                    bbox=dict(boxstyle='circle,pad=0.3', facecolor='red', alpha=0.8)\n",
    "                )\n",
    "        \n",
    "        ax.set_title('Mapa de Reportes de Alta Prioridad', fontsize=14, pad=20)\n",
    "        ax.set_xlabel('Longitud', fontsize=12)\n",
    "        ax.set_ylabel('Latitud', fontsize=12)\n",
    "        \n",
    "        cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "        cbar.set_label('Cluster ID (-1 = Ruido)', fontsize=10)\n",
    "        \n",
    "        if not analisis_zonas.empty:\n",
    "            ax.legend(loc='upper right', fontsize=10)\n",
    "        \n",
    "        ax.grid(True, alpha=0.2)\n",
    "        \n",
    "        # Informaci√≥n estad√≠stica\n",
    "        n_reportes_en_zonas = len(df_resultados[df_resultados['cluster'] != -1])\n",
    "        cobertura = (n_reportes_en_zonas / len(df_resultados) * 100) if len(df_resultados) > 0 else 0\n",
    "        info_text = f\"Reportes: {len(df_resultados):,} | Zonas: {n_clusters} | Cobertura: {cobertura:.1f}%\"\n",
    "        \n",
    "        ax.text(\n",
    "            0.02, 0.02, info_text, \n",
    "            transform=ax.transAxes, \n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No hay datos para mostrar', ha='center', va='center', fontsize=16)\n",
    "        ax.set_title('Sin datos disponibles')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(grafico_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()  # Mostrar en el notebook\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c05fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_zonas_criticas_avanzado(input_csv, auto_params=True, eps=None, min_samples=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Modelo avanzado de detecci√≥n de zonas cr√≠ticas con:\n",
    "    - Ingenier√≠a de caracter√≠sticas avanzada\n",
    "    - Optimizaci√≥n autom√°tica de par√°metros\n",
    "    - Validaci√≥n de calidad con m√©tricas\n",
    "    - An√°lisis temporal y categ√≥rico\n",
    "    - Visualizaci√≥n completa\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "\n",
    "    try:\n",
    "        log_print(f\"\\n=== MODELO AVANZADO DE DETECCI√ìN DE ZONAS CR√çTICAS ===\")\n",
    "        log_print(f\"[ETAPA 1/6] Cargando y validando datos...\")\n",
    "\n",
    "        if not os.path.exists(input_csv):\n",
    "            raise FileNotFoundError(f\"Archivo no encontrado: {input_csv}\")\n",
    "\n",
    "        df = pd.read_csv(input_csv)\n",
    "        log_print(f\"‚úì Datos cargados. Total de registros: {len(df)}\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"El archivo CSV est√° vac√≠o\")\n",
    "        \n",
    "        columnas_requeridas = ['latitud', 'longitud', 'prioridad']\n",
    "        columnas_faltantes = [col for col in columnas_requeridas if col not in df.columns]\n",
    "        if columnas_faltantes:\n",
    "            raise ValueError(f\"Columnas faltantes: {columnas_faltantes}\")\n",
    "\n",
    "        df_alta = df[df['prioridad'] == 'alta'].copy()\n",
    "        log_print(f\"‚úì Reportes de alta prioridad: {len(df_alta)}\")\n",
    "        \n",
    "        if len(df_alta) < 10:\n",
    "            raise ValueError(f\"Insuficientes reportes de alta prioridad: {len(df_alta)} (m√≠nimo 10)\")\n",
    "\n",
    "        # Limpieza de datos\n",
    "        df_alta = df_alta[\n",
    "            (df_alta['latitud'].notna()) & \n",
    "            (df_alta['longitud'].notna()) &\n",
    "            (df_alta['latitud'] != 'Latitud no especificada') &\n",
    "            (df_alta['longitud'] != 'Longitud no especificada')\n",
    "        ].copy()\n",
    "        \n",
    "        df_alta['latitud'] = pd.to_numeric(df_alta['latitud'], errors='coerce')\n",
    "        df_alta['longitud'] = pd.to_numeric(df_alta['longitud'], errors='coerce')\n",
    "        df_alta = df_alta.dropna(subset=['latitud', 'longitud'])\n",
    "        \n",
    "        log_print(f\"‚úì Datos limpios: {len(df_alta)} reportes v√°lidos\")\n",
    "\n",
    "        return df_alta  # Retornamos los datos procesados para continuar en la siguiente celda\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_details = {\n",
    "            \"error\": str(e),\n",
    "            \"tipo\": type(e).__name__,\n",
    "            \"archivo\": input_csv,\n",
    "            \"traceback\": traceback.format_exc()\n",
    "        }\n",
    "        log_print(f\"\\n‚ùå ERROR CR√çTICO:\\n{json.dumps(error_details, indent=2)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_caracteristicas_avanzadas(df_alta, verbose=True):\n",
    "    \"\"\"\n",
    "    Genera caracter√≠sticas avanzadas para el modelo\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 2/6] Generando caracter√≠sticas avanzadas...\")\n",
    "    \n",
    "    df_features = df_alta[['latitud', 'longitud']].copy()\n",
    "    \n",
    "    # Caracter√≠sticas temporales\n",
    "    if 'fecha_creacion' in df_alta.columns:\n",
    "        df_alta['fecha_creacion'] = pd.to_datetime(df_alta['fecha_creacion'], errors='coerce')\n",
    "        df_features['hora_reporte'] = df_alta['fecha_creacion'].dt.hour\n",
    "        df_features['dia_semana'] = df_alta['fecha_creacion'].dt.dayofweek\n",
    "        df_features['mes'] = df_alta['fecha_creacion'].dt.month\n",
    "        df_features['es_horario_laboral'] = ((df_features['hora_reporte'] >= 8) & \n",
    "                                           (df_features['hora_reporte'] <= 18)).astype(int)\n",
    "        df_features['es_fin_semana'] = (df_features['dia_semana'] >= 5).astype(int)\n",
    "    \n",
    "    # Caracter√≠sticas categ√≥ricas\n",
    "    if 'categoria_id' in df_alta.columns:\n",
    "        df_alta['categoria_id'] = pd.to_numeric(df_alta['categoria_id'], errors='coerce').fillna(0)\n",
    "        le_categoria = LabelEncoder()\n",
    "        df_features['categoria_encoded'] = le_categoria.fit_transform(df_alta['categoria_id'])\n",
    "    \n",
    "    if 'estado' in df_alta.columns:\n",
    "        le_estado = LabelEncoder()\n",
    "        df_features['estado_encoded'] = le_estado.fit_transform(df_alta['estado'].fillna('nuevo'))\n",
    "    \n",
    "    # Caracter√≠sticas espaciales\n",
    "    coords_temp = df_features[['latitud', 'longitud']].values\n",
    "    k_neighbors = min(10, len(coords_temp) - 1)\n",
    "    \n",
    "    if k_neighbors > 0:\n",
    "        nbrs = NearestNeighbors(n_neighbors=k_neighbors, radius=0.01).fit(coords_temp)\n",
    "        distances, indices = nbrs.kneighbors(coords_temp)\n",
    "        df_features['densidad_local'] = np.mean(distances, axis=1)\n",
    "        df_features['num_vecinos_cercanos'] = np.sum(distances < 0.005, axis=1)\n",
    "    \n",
    "    # Distancia al centro\n",
    "    centro_ciudad = [df_features['latitud'].mean(), df_features['longitud'].mean()]\n",
    "    df_features['distancia_centro'] = np.sqrt(\n",
    "        (df_features['latitud'] - centro_ciudad[0])**2 + \n",
    "        (df_features['longitud'] - centro_ciudad[1])**2\n",
    "    )\n",
    "    \n",
    "    # Normalizaci√≥n de coordenadas\n",
    "    df_features['lat_normalizada'] = (df_features['latitud'] - df_features['latitud'].min()) / (df_features['latitud'].max() - df_features['latitud'].min())\n",
    "    df_features['lng_normalizada'] = (df_features['longitud'] - df_features['longitud'].min()) / (df_features['longitud'].max() - df_features['longitud'].min())\n",
    "    \n",
    "    log_print(f\"‚úì Caracter√≠sticas generadas: {df_features.shape[1]} variables\")\n",
    "    \n",
    "    # Escalado de caracter√≠sticas\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(df_features)\n",
    "    log_print(f\"‚úì Caracter√≠sticas normalizadas\")\n",
    "    \n",
    "    return df_features, features_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizar_parametros_dbscan(features_scaled, auto_params=True, eps=None, min_samples=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimiza los par√°metros de DBSCAN\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 3/6] Optimizando par√°metros...\")\n",
    "    \n",
    "    if auto_params:\n",
    "        # An√°lisis de k-distance\n",
    "        k = min(max(4, len(features_scaled) // 50), 15)\n",
    "        nbrs = NearestNeighbors(n_neighbors=k).fit(features_scaled)\n",
    "        distances, indices = nbrs.kneighbors(features_scaled)\n",
    "        k_distances = np.sort(distances[:, k-1], axis=0)\n",
    "        \n",
    "        eps_candidates = [\n",
    "            np.percentile(k_distances, 85),\n",
    "            np.percentile(k_distances, 90),\n",
    "            np.percentile(k_distances, 95)\n",
    "        ]\n",
    "        \n",
    "        min_samples_candidates = [\n",
    "            max(3, len(features_scaled) // 100),\n",
    "            max(5, len(features_scaled) // 50),\n",
    "            max(7, len(features_scaled) // 30)\n",
    "        ]\n",
    "        \n",
    "        log_print(f\"‚úì Candidatos eps: {[f'{x:.4f}' for x in eps_candidates]}\")\n",
    "        log_print(f\"‚úì Candidatos min_samples: {min_samples_candidates}\")\n",
    "    else:\n",
    "        eps_candidates = [eps or 0.5]\n",
    "        min_samples_candidates = [min_samples or 5]\n",
    "        log_print(f\"‚úì Par√°metros manuales: eps={eps_candidates[0]}, min_samples={min_samples_candidates[0]}\")\n",
    "    \n",
    "    return eps_candidates, min_samples_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_clustering(features_scaled, eps_candidates, min_samples_candidates, verbose=True):\n",
    "    \"\"\"\n",
    "    Ejecuta el clustering DBSCAN con validaci√≥n\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 4/6] Ejecutando clustering con validaci√≥n...\")\n",
    "    \n",
    "    best_score = -1\n",
    "    best_labels = None\n",
    "    best_params = None\n",
    "    best_dbscan = None\n",
    "    resultados_pruebas = []\n",
    "    \n",
    "    for test_eps in eps_candidates:\n",
    "        for test_min_samples in min_samples_candidates:\n",
    "            try:\n",
    "                dbscan_test = DBSCAN(eps=test_eps, min_samples=test_min_samples, metric='euclidean')\n",
    "                labels_test = dbscan_test.fit_predict(features_scaled)\n",
    "                \n",
    "                n_clusters_test = len(set(labels_test)) - (1 if -1 in labels_test else 0)\n",
    "                n_noise_test = list(labels_test).count(-1)\n",
    "                \n",
    "                if n_clusters_test >= 1 and n_clusters_test <= len(features_scaled) // 3:\n",
    "                    if n_clusters_test == 1:\n",
    "                        score = 1.0 - (n_noise_test / len(labels_test))\n",
    "                    else:\n",
    "                        try:\n",
    "                            score = silhouette_score(features_scaled, labels_test)\n",
    "                        except:\n",
    "                            score = 0.0\n",
    "                    \n",
    "                    resultado = {\n",
    "                        'eps': test_eps,\n",
    "                        'min_samples': test_min_samples,\n",
    "                        'n_clusters': n_clusters_test,\n",
    "                        'n_noise': n_noise_test,\n",
    "                        'score': score\n",
    "                    }\n",
    "                    resultados_pruebas.append(resultado)\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_labels = labels_test\n",
    "                        best_params = (test_eps, test_min_samples)\n",
    "                        best_dbscan = dbscan_test\n",
    "                        \n",
    "            except Exception as e:\n",
    "                log_print(f\"  Error con eps={test_eps:.4f}, min_samples={test_min_samples}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Configuraci√≥n fallback si no se encuentra una buena soluci√≥n\n",
    "    if best_labels is None:\n",
    "        log_print(\"  ‚ö†  Aplicando configuraci√≥n fallback...\")\n",
    "        fallback_eps = 0.5\n",
    "        fallback_min_samples = max(3, min(10, len(features_scaled) // 20))\n",
    "        best_dbscan = DBSCAN(eps=fallback_eps, min_samples=fallback_min_samples)\n",
    "        best_labels = best_dbscan.fit_predict(features_scaled)\n",
    "        best_params = (fallback_eps, fallback_min_samples)\n",
    "        best_score = 0.0\n",
    "\n",
    "    n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)\n",
    "    n_noise = list(best_labels).count(-1)\n",
    "    \n",
    "    log_print(f\"‚úì Mejor configuraci√≥n encontrada:\")\n",
    "    log_print(f\"  - Par√°metros: eps={best_params[0]:.4f}, min_samples={best_params[1]}\")\n",
    "    log_print(f\"  - Zonas cr√≠ticas detectadas: {n_clusters}\")\n",
    "    log_print(f\"  - Reportes aislados: {n_noise}\")\n",
    "    log_print(f\"  - Score de calidad: {best_score:.3f}\")\n",
    "    \n",
    "    return best_dbscan, best_labels, best_params, best_score, resultados_pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ca097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_resultados(df_alta, best_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Analiza los resultados del clustering\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 5/6] Analizando resultados...\")\n",
    "    \n",
    "    df_resultados = df_alta.copy()\n",
    "    df_resultados['cluster'] = best_labels\n",
    "    df_resultados['es_zona_critica'] = df_resultados['cluster'] != -1\n",
    "    \n",
    "    zonas_criticas = df_resultados[df_resultados['es_zona_critica']]\n",
    "    \n",
    "    if not zonas_criticas.empty:\n",
    "        # Estad√≠sticas b√°sicas por zona\n",
    "        stats_basicos = zonas_criticas.groupby('cluster').agg({\n",
    "            'latitud': ['mean', 'std', 'min', 'max', 'count'],\n",
    "            'longitud': ['mean', 'std', 'min', 'max'],\n",
    "        }).round(6)\n",
    "        \n",
    "        # Estad√≠sticas adicionales\n",
    "        stats_adicionales = {}\n",
    "        \n",
    "        if 'categoria_id' in zonas_criticas.columns:\n",
    "            stats_adicionales['categoria_dominante'] = zonas_criticas.groupby('cluster')['categoria_id'].agg(\n",
    "                lambda x: Counter(x).most_common(1)[0][0] if len(x) > 0 else None\n",
    "            )\n",
    "        \n",
    "        if 'estado' in zonas_criticas.columns:\n",
    "            stats_adicionales['estado_dominante'] = zonas_criticas.groupby('cluster')['estado'].agg(\n",
    "                lambda x: Counter(x).most_common(1)[0][0] if len(x) > 0 else None\n",
    "            )\n",
    "        \n",
    "        if 'fecha_creacion' in zonas_criticas.columns:\n",
    "            stats_adicionales['periodo'] = zonas_criticas.groupby('cluster')['fecha_creacion'].agg(['min', 'max'])\n",
    "        \n",
    "        # Crear DataFrame de an√°lisis\n",
    "        analisis_zonas = pd.DataFrame({\n",
    "            'lat_centro': stats_basicos['latitud']['mean'],\n",
    "            'lng_centro': stats_basicos['longitud']['mean'],\n",
    "            'lat_std': stats_basicos['latitud']['std'].fillna(0),\n",
    "            'lng_std': stats_basicos['longitud']['std'].fillna(0),\n",
    "            'num_reportes': stats_basicos['latitud']['count'],\n",
    "            'lat_min': stats_basicos['latitud']['min'],\n",
    "            'lat_max': stats_basicos['latitud']['max'],\n",
    "            'lng_min': stats_basicos['longitud']['min'],\n",
    "            'lng_max': stats_basicos['longitud']['max']\n",
    "        })\n",
    "        \n",
    "        # Agregar estad√≠sticas adicionales\n",
    "        for key, values in stats_adicionales.items():\n",
    "            if isinstance(values, pd.DataFrame):\n",
    "                for col in values.columns:\n",
    "                    analisis_zonas[f'{key}_{col}'] = values[col]\n",
    "            else:\n",
    "                analisis_zonas[key] = values\n",
    "        \n",
    "        # M√©tricas de criticidad\n",
    "        analisis_zonas['densidad'] = analisis_zonas['num_reportes']\n",
    "        analisis_zonas['area_cobertura'] = (analisis_zonas['lat_max'] - analisis_zonas['lat_min']) * (analisis_zonas['lng_max'] - analisis_zonas['lng_min'])\n",
    "        analisis_zonas['compacidad'] = analisis_zonas['lat_std'] + analisis_zonas['lng_std']\n",
    "        \n",
    "        analisis_zonas['score_criticidad'] = (\n",
    "            (analisis_zonas['densidad'] / analisis_zonas['densidad'].max()) * 0.5 +\n",
    "            (1 / (analisis_zonas['compacidad'] + 0.001)) * 0.3 +\n",
    "            (1 / (analisis_zonas['area_cobertura'] + 0.001)) * 0.2\n",
    "        )\n",
    "        \n",
    "        # Clasificaci√≥n de criticidad\n",
    "        if len(analisis_zonas) > 1:\n",
    "            try:\n",
    "                analisis_zonas['nivel_criticidad'] = pd.qcut(\n",
    "                    analisis_zonas['score_criticidad'], \n",
    "                    q=min(3, len(analisis_zonas)),\n",
    "                    labels=['Media', 'Alta', 'Cr√≠tica'][:min(3, len(analisis_zonas))],\n",
    "                    duplicates='drop'\n",
    "                )\n",
    "            except:\n",
    "                analisis_zonas['nivel_criticidad'] = 'Alta'\n",
    "        else:\n",
    "            analisis_zonas['nivel_criticidad'] = 'Alta'\n",
    "        \n",
    "        log_print(f\"‚úì An√°lisis completado - Resumen por zona:\")\n",
    "        for idx, zona in analisis_zonas.iterrows():\n",
    "            categoria_info = f\", Cat. dominante: {zona.get('categoria_dominante', 'N/A')}\" if 'categoria_dominante' in zona else \"\"\n",
    "            log_print(f\"  Zona {idx}: {zona['num_reportes']} reportes, Centro: ({zona['lat_centro']:.4f}, {zona['lng_centro']:.4f})\")\n",
    "            log_print(f\"    Nivel: {zona['nivel_criticidad']}, Score: {zona['score_criticidad']:.2f}{categoria_info}\")\n",
    "    else:\n",
    "        analisis_zonas = pd.DataFrame()\n",
    "        log_print(\"  ‚Ñπ  No se identificaron zonas cr√≠ticas\")\n",
    "    \n",
    "    return df_resultados, analisis_zonas, zonas_criticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4726566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_modelo_y_generar_visualizacion(best_dbscan, scaler, df_features, analisis_zonas, \n",
    "                                          resultados_pruebas, best_score, best_params, \n",
    "                                          df_resultados, n_clusters, verbose=True):\n",
    "    \"\"\"\n",
    "    Guarda el modelo y genera la visualizaci√≥n final\n",
    "    \"\"\"\n",
    "    def log_print(mensaje):\n",
    "        if verbose:\n",
    "            print(mensaje)\n",
    "    \n",
    "    log_print(\"[ETAPA 6/6] Generando visualizaci√≥n y guardando modelo...\")\n",
    "    \n",
    "    # Crear directorios\n",
    "    os.makedirs(\"modelos\", exist_ok=True)\n",
    "    os.makedirs(\"graficos\", exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # M√©tricas de validaci√≥n\n",
    "    zonas_criticas = df_resultados[df_resultados['cluster'] != -1]\n",
    "    n_noise = list(df_resultados['cluster']).count(-1)\n",
    "    \n",
    "    metricas_validacion = {\n",
    "        'silhouette_score': float(best_score),\n",
    "        'n_clusters': int(n_clusters),\n",
    "        'n_noise': int(n_noise),\n",
    "        'coverage_ratio': float(len(zonas_criticas) / len(df_resultados)) if len(df_resultados) > 0 else 0.0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if n_clusters > 1:\n",
    "            features_scaled = StandardScaler().fit_transform(df_features)\n",
    "            metricas_validacion['calinski_harabasz'] = float(\n",
    "                calinski_harabasz_score(features_scaled, df_resultados['cluster'])\n",
    "            )\n",
    "    except:\n",
    "        metricas_validacion['calinski_harabasz'] = 0.0\n",
    "    \n",
    "    # Preparar datos del modelo\n",
    "    modelo_data = {\n",
    "        'dbscan': best_dbscan,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': df_features.columns.tolist(),\n",
    "        'zonas_criticas': analisis_zonas,\n",
    "        'resultados_optimizacion': resultados_pruebas,\n",
    "        'metricas_validacion': metricas_validacion,\n",
    "        'metadata': {\n",
    "            'fecha_entrenamiento': timestamp,\n",
    "            'version': 'solo_no_supervisado_v1',\n",
    "            'caracteristicas_usadas': len(df_features.columns),\n",
    "            'parametros_finales': {\n",
    "                'eps': best_params[0],\n",
    "                'min_samples': best_params[1]\n",
    "            },\n",
    "            'estadisticas': {\n",
    "                'reportes_procesados': len(df_resultados),\n",
    "                'zonas_criticas_detectadas': n_clusters,\n",
    "                'reportes_en_zonas_criticas': len(zonas_criticas),\n",
    "                'reportes_aislados': n_noise\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Rutas de archivos\n",
    "    modelo_path = os.path.join(\"modelos\", f\"zonas_criticas_avanzado_{timestamp}.joblib\")\n",
    "    grafico_path = os.path.join(\"graficos\", f\"zonas_criticas_avanzado_{timestamp}.png\")\n",
    "    \n",
    "    # Generar visualizaci√≥n\n",
    "    generar_visualizacion_completa(df_resultados, analisis_zonas, n_clusters, grafico_path)\n",
    "    log_print(f\"‚úì Visualizaci√≥n completa generada: {grafico_path}\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    joblib.dump(modelo_data, modelo_path)\n",
    "    log_print(f\"‚úÖ Modelo completo guardado: {modelo_path}\")\n",
    "    \n",
    "    # Resumen final\n",
    "    log_print(f\"\\n=== ENTRENAMIENTO COMPLETADO EXITOSAMENTE ===\")\n",
    "    log_print(f\"Zonas cr√≠ticas detectadas: {n_clusters}\")\n",
    "    log_print(f\"Calidad del modelo: {best_score:.3f}\")\n",
    "    log_print(f\"Cobertura de reportes: {(len(zonas_criticas)/len(df_resultados)*100):.1f}%\")\n",
    "    \n",
    "    return modelo_path, grafico_path, metricas_validacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea275018",
   "metadata": {},
   "source": [
    "## Funci√≥n Principal Integrada\n",
    "\n",
    "Esta funci√≥n integra todo el pipeline de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd976bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_completo(input_csv, auto_params=True, eps=None, min_samples=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal que ejecuta todo el pipeline de entrenamiento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Etapa 1: Cargar y validar datos\n",
    "        df_alta = entrenar_modelo_zonas_criticas_avanzado(input_csv, auto_params, eps, min_samples, verbose)\n",
    "        \n",
    "        # Etapa 2: Generar caracter√≠sticas\n",
    "        df_features, features_scaled, scaler = generar_caracteristicas_avanzadas(df_alta, verbose)\n",
    "        \n",
    "        # Etapa 3: Optimizar par√°metros\n",
    "        eps_candidates, min_samples_candidates = optimizar_parametros_dbscan(\n",
    "            features_scaled, auto_params, eps, min_samples, verbose\n",
    "        )\n",
    "        \n",
    "        # Etapa 4: Ejecutar clustering\n",
    "        best_dbscan, best_labels, best_params, best_score, resultados_pruebas = ejecutar_clustering(\n",
    "            features_scaled, eps_candidates, min_samples_candidates, verbose\n",
    "        )\n",
    "        \n",
    "        # Etapa 5: Analizar resultados\n",
    "        df_resultados, analisis_zonas, zonas_criticas = analizar_resultados(df_alta, best_labels, verbose)\n",
    "        \n",
    "        # Calcular n√∫mero de clusters\n",
    "        n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)\n",
    "        \n",
    "        # Etapa 6: Guardar modelo y generar visualizaci√≥n\n",
    "        modelo_path, grafico_path, metricas_validacion = guardar_modelo_y_generar_visualizacion(\n",
    "            best_dbscan, scaler, df_features, analisis_zonas, resultados_pruebas, \n",
    "            best_score, best_params, df_resultados, n_clusters, verbose\n",
    "        )\n",
    "        \n",
    "        # Retornar resultados\n",
    "        resultado = {\n",
    "            \"success\": True,\n",
    "            \"modelo_path\": modelo_path,\n",
    "            \"grafico_path\": grafico_path,\n",
    "            \"version\": \"solo_no_supervisado_v1\",\n",
    "            \"metricas\": metricas_validacion,\n",
    "            \"analisis_zonas\": analisis_zonas,\n",
    "            \"df_resultados\": df_resultados\n",
    "        }\n",
    "        \n",
    "        return resultado\n",
    "        \n",
    "    except Exception as e:\n",
    "        resultado_error = {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"version\": \"solo_no_supervisado_v1\"\n",
    "        }\n",
    "        print(f\"Error en el entrenamiento: {resultado_error}\")\n",
    "        return resultado_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f10bcc",
   "metadata": {},
   "source": [
    "## Ejemplo de Uso\n",
    "\n",
    "Aqu√≠ puedes ejecutar el modelo con tus datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso del modelo\n",
    "# Cambia 'tu_archivo.csv' por la ruta de tu archivo de datos\n",
    "\n",
    "# Configuraci√≥n\n",
    "archivo_datos = 'tu_archivo.csv'  # Cambia por tu archivo\n",
    "usar_optimizacion_automatica = True  # True para optimizaci√≥n autom√°tica, False para par√°metros manuales\n",
    "eps_manual = None  # Solo si usar_optimizacion_automatica = False\n",
    "min_samples_manual = None  # Solo si usar_optimizacion_automatica = False\n",
    "\n",
    "# Ejecutar el modelo\n",
    "if os.path.exists(archivo_datos):\n",
    "    print(\"Iniciando entrenamiento del modelo...\")\n",
    "    resultado = entrenar_modelo_completo(\n",
    "        input_csv=archivo_datos,\n",
    "        auto_params=usar_optimizacion_automatica,\n",
    "        eps=eps_manual,\n",
    "        min_samples=min_samples_manual,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    if resultado['success']:\n",
    "        print(\"\\nüéâ ¬°Entrenamiento completado exitosamente!\")\n",
    "        print(f\"üìä Modelo guardado en: {resultado['modelo_path']}\")\n",
    "        print(f\"üìà Gr√°fico generado en: {resultado['grafico_path']}\")\n",
    "        \n",
    "        # Mostrar m√©tricas\n",
    "        print(\"\\nüìã M√©tricas del modelo:\")\n",
    "        for metrica, valor in resultado['metricas'].items():\n",
    "            print(f\"  {metrica}: {valor}\")\n",
    "        \n",
    "        # Mostrar resumen de zonas cr√≠ticas\n",
    "        if not resultado['analisis_zonas'].empty:\n",
    "            print(\"\\nüó∫Ô∏è Resumen de zonas cr√≠ticas:\")\n",
    "            print(resultado['analisis_zonas'][['num_reportes', 'nivel_criticidad', 'score_criticidad']].head())\n",
    "    else:\n",
    "        print(f\"‚ùå Error en el entrenamiento: {resultado['error']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Archivo no encontrado: {archivo_datos}\")\n",
    "    print(\"üí° Aseg√∫rate de que el archivo CSV existe y contiene las columnas: 'latitud', 'longitud', 'prioridad'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718747ba",
   "metadata": {},
   "source": [
    "## Carga y An√°lisis de Modelo Previamente Entrenado\n",
    "\n",
    "Si ya tienes un modelo entrenado, puedes cargarlo y analizarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d456ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_y_analizar_modelo(modelo_path):\n",
    "    \"\"\"\n",
    "    Carga un modelo previamente entrenado y muestra su an√°lisis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar modelo\n",
    "        modelo_data = joblib.load(modelo_path)\n",
    "        \n",
    "        print(f\"üìÇ Modelo cargado desde: {modelo_path}\")\n",
    "        print(f\"üìÖ Fecha de entrenamiento: {modelo_data['metadata']['fecha_entrenamiento']}\")\n",
    "        print(f\"üîß Versi√≥n: {modelo_data['metadata']['version']}\")\n",
    "        \n",
    "        # Mostrar estad√≠sticas\n",
    "        stats = modelo_data['metadata']['estadisticas']\n",
    "        print(\"\\nüìä Estad√≠sticas del modelo:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Mostrar m√©tricas de validaci√≥n\n",
    "        print(\"\\nüìà M√©tricas de validaci√≥n:\")\n",
    "        for key, value in modelo_data['metricas_validacion'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Mostrar zonas cr√≠ticas\n",
    "        if not modelo_data['zonas_criticas'].empty:\n",
    "            print(\"\\nüó∫Ô∏è Zonas cr√≠ticas detectadas:\")\n",
    "            zonas = modelo_data['zonas_criticas']\n",
    "            for idx, zona in zonas.iterrows():\n",
    "                print(f\"  Zona {idx}: {zona['num_reportes']} reportes, Nivel: {zona['nivel_criticidad']}\")\n",
    "                print(f\"    Centro: ({zona['lat_centro']:.4f}, {zona['lng_centro']:.4f})\")\n",
    "                print(f\"    Score de criticidad: {zona['score_criticidad']:.2f}\")\n",
    "        \n",
    "        return modelo_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando el modelo: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejemplo de uso\n",
    "# modelo_path = \"modelos/zonas_criticas_avanzado_20241212_143022.joblib\" \n",
    "# modelo_cargado = cargar_y_analizar_modelo(modelo_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
